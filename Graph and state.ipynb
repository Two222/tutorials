{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an investigation into graph and state, which could be turned into a real tutorial or appended to the existing \"pytorch for torchies\" tutorial.\n",
    "\n",
    "Also see https://discuss.pytorch.org/t/understanding-graphs-and-state/224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State/buffer clearing during backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaves a little different than I expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is straight from the \"pytorch for torchies\" tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "# backward on part of the graph\n",
    "z.backward(torch.range(1,4).view(2,2)) # grad_outp[1,2,3,4] * 6*(xi+2)\n",
    "print(x.grad)\n",
    "# out.backward() # fails cause buffers have been freed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'd expect this to fail too but it doesnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2,3), requires_grad=True)\n",
    "y = x.mean(dim=1).squeeze() + 3 # size (2,)\n",
    "z = y.pow(2).mean() # size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward(torch.ones(2))\n",
    "z.backward() # should fail! But only fails on second execution\n",
    "y.backward(torch.ones(2)) # still fine, though we're calling it for the second time\n",
    "z.backward() # this fails (finally!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My guess: it's not guaranteed that an error is raised on the second backward pass through part of the graph. But of course if we need to keep buffers on part of the graph, we have to supply retain_variables=True. Cause buffers *could* have been freed.\n",
    "\n",
    "Probably the specific simple operations for y (mean, add) don't need buffers for backward, while the `z=y.pow(2).mean()` does need a buffer to store the result of `y.pow(2)`. correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilary loss functions on small convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question is about when a new graph (and thus new state) is allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 3)\n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.conv1(x))\n",
    "        out2 = F.relu(self.conv2(out1))\n",
    "        return out1, out2\n",
    "net = Net()\n",
    "inp = Variable(torch.randn(1,1,6,6))\n",
    "inp2 = Variable(torch.randn(1,1,6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out1, out2 = net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "print(net.conv1.weight.grad)\n",
    "print(net.conv2.weight.grad)\n",
    "out1.backward(torch.ones(1,1,4,4), retain_variables=True) # out2.backward fails without the flag, as expected\n",
    "out2.backward(torch.ones(1,1,2,2))\n",
    "print(net.conv1.weight.grad)\n",
    "print(net.conv2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Everything as expected here.\n",
    "\n",
    "As explained in tutorial, two different inputs through a net will give different graphs, and thus hold different state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, out = net(inp)\n",
    "_, out2 = net(inp2)\n",
    "out.backward(torch.ones(1,1,2,2))\n",
    "out2.backward(torch.ones(1,1,2,2))\n",
    "# out.backward(torch.ones(1,1,2,2)) # fails as expected, buffers are freed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But two times the same variable, doesnt overwrite state in the same graph, but rather the two forward passes become separate graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, out = net(inp)\n",
    "_, out2 = net(inp) # same input this time\n",
    "out.backward(torch.ones(1,1,2,2))\n",
    "out2.backward(torch.ones(1,1,2,2)) # doesnt fail -> has a different state than the first fw pass?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem I see with this design is that often (during testing, or when you detach() to cut off gradients, or anytime you add an extra operation just for monitoring) there's just a fw-pass on part of the graph - so is that state then kept around forever and just starts consuming more memory on every new fw-pass of the same variable?\n",
    "\n",
    "I understand that the volatile flag is probably introduced for this problem and I see it's used during testing in most example code.\n",
    "\n",
    "But I think these are some examples where there's just fw-pass without `volatile` flag:\n",
    "\n",
    "+ `fake = netG(noise).detach()` to avoid bpropping through netG  https://github.com/pytorch/examples/blob/master/dcgan/main.py#L216\n",
    "+ test on non-volatile variables: https://github.com/pytorch/examples/blob/master/super_resolution/main.py#L74\n",
    "+ If you finetune only top layers of a feedforward net, bottom layers see only fw-passes\n",
    "\n",
    "But in general, if I understand this design correctly, this means anytime you have a part of a network which isn't backpropped through, you need to supply volatile flag? Then when you use that intermediate volatile variable in another part of the network which is backpropped through, you need to re-wrap and turn volatile off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 sequentials from same list of modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we make a separate Sequential() from same list of modules, and input same variable, we get same graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question actually becomes irrelevant if above observation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mods=[\n",
    " nn.Conv2d(1, 1, 3),\n",
    " nn.ReLU(),\n",
    " nn.Conv2d(1, 1, 3),\n",
    " nn.ReLU(),\n",
    " nn.Conv2d(1, 1, 3)]\n",
    "seq1 = nn.Sequential(*mods)\n",
    "seq2 = nn.Sequential(*mods)\n",
    "inp  = Variable(torch.randn(1,1,7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out1=seq1(inp).squeeze()\n",
    "out2=seq2(inp).squeeze()\n",
    "net.zero_grad()\n",
    "out1.backward()\n",
    "out2.backward() # doesnt fail, so is separate graph with separate state.\n",
    "# out1.backward() # fails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
